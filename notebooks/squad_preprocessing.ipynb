{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Squad Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import spacy\n",
    "import re\n",
    "import random\n",
    "from ftfy import fix_text\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from preprocessing.squad import preprocess\n",
    "from util.Tokenizer import Tokenizer\n",
    "from util.util import load_json\n",
    "from util.squad import extract_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/squad/train-v1.1.json'\n",
    "dev_path = '../data/squad/dev-v1.1.json'\n",
    "vocab_path = '../data/glove/vocab.json'\n",
    "oov_token = '<oov>'\n",
    "trainable_words = [oov_token]\n",
    "max_words = 150000\n",
    "context_limit = 400\n",
    "question_limit = 50\n",
    "ans_limit = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_json(train_path)\n",
    "dev = load_json(dev_path)\n",
    "vocab = set(load_json(vocab_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dims\n",
    "word_embed_dims = 300\n",
    "# char_embed_dims = 200\n",
    "char_embed_dims = 64\n",
    "# Other params\n",
    "CONTEXT_LIMIT = 400\n",
    "QUESTION_LIMIT = 50\n",
    "# Regexes\n",
    "apostrophe = re.compile(r\"('')\")\n",
    "apostrophe_like = re.compile(r\"(``)\")\n",
    "\n",
    "\n",
    "def convert_idx(text, tokens):\n",
    "    current = 0\n",
    "    spans = []\n",
    "\n",
    "    for token in tokens:\n",
    "        current = text.find(token, current)\n",
    "        \n",
    "        if current < 0:\n",
    "\n",
    "            print(\"Token {} cannot be found\".format(token))\n",
    "            raise ValueError('Could not find token.')\n",
    "        spans.append((current, current + len(token)))\n",
    "        current += len(token)\n",
    "    return spans\n",
    "\n",
    "\n",
    "def fit_on_squad(data_set, tokenizer):\n",
    "    for data in tqdm(data_set['data']):\n",
    "        for question_answer in data['paragraphs']:\n",
    "            context = fix_text(question_answer['context'])\n",
    "            context = apostrophe.sub('\" ', context)\n",
    "            context = apostrophe_like.sub('\" ', context)\n",
    "\n",
    "            tokenizer.fit_on_texts(context)\n",
    "\n",
    "            for qa in question_answer['qas']:\n",
    "                ques = fix_text(qa['question'].replace(\"''\", '\" ').replace(\"``\", '\" '))\n",
    "                tokenizer.fit_on_texts(ques)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def pre_process(data_set, tokenizer, context_limit, question_limit):\n",
    "    indexed = []\n",
    "    answers = {}\n",
    "    contexts = {}\n",
    "    context_id = 1\n",
    "    answer_id = 1\n",
    "\n",
    "    for data in tqdm(data_set['data']):\n",
    "        for question_answer in data['paragraphs']:\n",
    "            context = fix_text(question_answer['context'])\n",
    "            context = apostrophe.sub('\" ', context)\n",
    "            context = apostrophe_like.sub('\" ', context)\n",
    "            context_words, context_chars, context_length = tokenizer.texts_to_sequences(context, max_words=context_limit,\n",
    "                                                                           numpy=False, pad=False)\n",
    "            # Tokenizer wraps in a list (len==1) so we need the last entry\n",
    "            context_words = context_words[-1]\n",
    "            context_chars = context_chars[-1]\n",
    "\n",
    "            # Skip if outside of context limit\n",
    "            if context_length[-1] > context_limit:\n",
    "                continue\n",
    "\n",
    "            spans = convert_idx(context, tokenizer.tokenize(context))\n",
    "\n",
    "            for qa in question_answer['qas']:\n",
    "                question = fix_text(qa['question'].replace(\"''\", '\" ').replace(\"``\", '\" '))\n",
    "                question_words, question_chars, question_length = tokenizer.texts_to_sequences(question,\n",
    "                                                                                               max_words=question_limit,\n",
    "                                                                                               numpy=False, pad=False)\n",
    "                # Tokenizer wraps in a list (len==1) so we need the last entry\n",
    "                question_words = question_words[-1]\n",
    "                question_chars = question_chars[-1]\n",
    "\n",
    "                # Skip if its outside of the limits.\n",
    "                if question_length[-1] > question_limit:\n",
    "                    continue\n",
    "\n",
    "                answer_starts, answer_ends = [], []\n",
    "                answer_texts = []\n",
    "\n",
    "                for answer in qa['answers']:\n",
    "                    answer_text = fix_text(answer['text'])\n",
    "                    answer_start = answer['answer_start']\n",
    "                    answer_end = answer_start + len(answer_text)\n",
    "                    answer_texts.append(answer_text)\n",
    "                    answer_span = []\n",
    "\n",
    "                    for i, span in enumerate(spans):\n",
    "                        if not (answer_end <= span[0] or answer_start >= span[1]):\n",
    "                            answer_span.append(i)\n",
    "\n",
    "                    answer_starts.append(answer_span[0])\n",
    "                    answer_ends.append(answer_span[-1])\n",
    "                    \n",
    "                if (answer_ends[-1] - answer_starts[-1]) > ans_limit:\n",
    "                    continue\n",
    "\n",
    "                indexed.append({\n",
    "                    'context_words': context_words,\n",
    "                    'context_chars': context_chars,\n",
    "                    'question_words': question_words,\n",
    "                    'question_chars': question_chars,\n",
    "                    'answer_starts': answer_starts[-1],\n",
    "                    'answer_ends': answer_ends[-1],\n",
    "                    'answer_id': answer_id,\n",
    "                })\n",
    "                \n",
    "                answers[answer_id] = {\n",
    "                    'id': qa['id'],\n",
    "                    'answer_id': answer_id,\n",
    "                    'context_id': context_id,\n",
    "                    'answers': answer_texts,\n",
    "                }\n",
    "\n",
    "                answer_id += 1\n",
    "\n",
    "            contexts[context_id] = {\n",
    "                'id': context_id,\n",
    "                'context': context,\n",
    "                'word_spans': spans,\n",
    "            }\n",
    "\n",
    "            context_id += 1\n",
    "\n",
    "    random.shuffle(indexed)\n",
    "    print(\"{} questions in total\".format(len(indexed)))\n",
    "    return indexed, contexts, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 442/442 [02:07<00:00,  3.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [00:14<00:00,  3.24it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(max_words=max_words + 1,\n",
    "                          vocab=vocab,\n",
    "                          lower=False,\n",
    "                          oov_token=oov_token,\n",
    "                          min_word_occurrence=-1,\n",
    "                          min_char_occurrence=-1,\n",
    "                          trainable_words=trainable_words,\n",
    "                          filters=set())  # Only filter stray whitespace\n",
    "\n",
    "print('Fitting...')\n",
    "tokenizer = fit_on_squad(train, tokenizer)\n",
    "tokenizer = fit_on_squad(dev, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 442/442 [03:40<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87353 questions in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [00:25<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10482 questions in total\n"
     ]
    }
   ],
   "source": [
    "print('Processing...')\n",
    "train_indexed, train_contexts, train_answers = pre_process(train, tokenizer,\n",
    "                                                                           context_limit=context_limit,\n",
    "                                                                           question_limit=question_limit)\n",
    "dev_indexed, dev_contexts, dev_answers = pre_process(dev, tokenizer,\n",
    "                                                                   context_limit=context_limit,\n",
    "                                                                   question_limit=question_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "\n",
    "def process_file(data, word_counter, char_counter):\n",
    "    examples = []\n",
    "    eval_examples = {}\n",
    "    total = 0\n",
    "    for article in tqdm(data[\"data\"]):\n",
    "        for para in article[\"paragraphs\"]:\n",
    "            context = para[\"context\"].replace(\n",
    "                    \"''\", '\" ').replace(\"``\", '\" ')\n",
    "            \n",
    "            context_tokens = word_tokenize(context)\n",
    "            context_chars = [list(token) for token in context_tokens]\n",
    "            \n",
    "            if len(context_tokens) > context_limit:\n",
    "                continue\n",
    "            \n",
    "            spans = convert_idx(context, context_tokens)\n",
    "            \n",
    "            for token in context_tokens:\n",
    "                word_counter[token] += len(para[\"qas\"])\n",
    "                for char in token:\n",
    "                    char_counter[char] += len(para[\"qas\"])\n",
    "            for qa in para[\"qas\"]:\n",
    "                total += 1\n",
    "                ques = qa[\"question\"].replace(\n",
    "                        \"''\", '\" ').replace(\"``\", '\" ')\n",
    "                ques_tokens = word_tokenize(ques)\n",
    "                ques_chars = [list(token) for token in ques_tokens]\n",
    "                \n",
    "                # Skip if its outside of the limits.\n",
    "                if len(ques_tokens) > question_limit:\n",
    "                    continue\n",
    "                \n",
    "                for token in ques_tokens:\n",
    "                    word_counter[token] += 1\n",
    "                    for char in token:\n",
    "                        char_counter[char] += 1\n",
    "                        \n",
    "                y1s, y2s = [], []\n",
    "                answer_texts = []\n",
    "                for answer in qa[\"answers\"]:\n",
    "                    answer_text = answer[\"text\"]\n",
    "                    answer_start = answer['answer_start']\n",
    "                    answer_end = answer_start + len(answer_text)\n",
    "                    answer_texts.append(answer_text)\n",
    "                    answer_span = []\n",
    "                    for idx, span in enumerate(spans):\n",
    "                        if not (answer_end <= span[0] or answer_start >= span[1]):\n",
    "                            answer_span.append(idx)\n",
    "                    y1, y2 = answer_span[0], answer_span[-1]\n",
    "                    y1s.append(y1)\n",
    "                    y2s.append(y2)\n",
    "                \n",
    "                if (y2s[-1] - y1s[-1]) > ans_limit:\n",
    "                    continue\n",
    "                \n",
    "                example = {\"context_tokens\": context_tokens, \"context_chars\": context_chars, \"ques_tokens\": ques_tokens,\n",
    "                               \"ques_chars\": ques_chars, \"y1s\": y1s[-1], \"y2s\": y2s[-1], \"id\": total}\n",
    "                examples.append(example)\n",
    "                eval_examples[str(total)] = {\n",
    "                        \"context\": context, \"spans\": spans, \"answers\": answer_texts, \"uuid\": qa[\"id\"]}\n",
    "    random.shuffle(examples)\n",
    "    print(\"{} questions in total\".format(len(examples)))\n",
    "    return examples, eval_examples\n",
    "\n",
    "\n",
    "def prepro():\n",
    "    word_counter, char_counter = Counter(), Counter()\n",
    "    train_examples, train_eval = process_file(train, word_counter, char_counter)\n",
    "    dev_examples, dev_eval = process_file(dev, word_counter, char_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 442/442 [02:01<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87358 questions in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [00:13<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10482 questions in total\n"
     ]
    }
   ],
   "source": [
    "prepro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from collections import OrderedDict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    'a': 5,\n",
    "    'b': 3,\n",
    "    'c': 16,\n",
    "    'z': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 5), ('b', 3), ('c', 16), ('z', 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(test_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('a', 5), ('c', 16), ('z', 1), ('b', 3)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrderedDict([(k,v) for k,v in test_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['a'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 1, 'd': 35, 'v': 14, 'z': 21})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['v'] += 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 35), ('z', 21), ('v', 14), ('a', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.most_common(150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-318bd64c9e42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'a'"
     ]
    }
   ],
   "source": [
    "test_dict.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_4 = SimpleNamespace(**test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(a=5, b=3, c=16, z=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_4.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
